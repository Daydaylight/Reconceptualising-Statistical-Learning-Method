# 统计学习的分类    
监督学习的本质：学习输入到输出的映射的统计规律。

无监督学习的本质：学习数据中的，统计规律或潜在结构。

强化学习的本质：学习最优的序贯决策。

## 概率模型与非概率模型

### 概率模型
决策树、朴素贝叶斯、隐马尔可夫模型、条件随机场、概率潜在语义分析、潜在狄利克雷分配、高斯混合模型。

    逻辑斯谛回归既可看作是概率模型，又可看作是非概率模型。
### 非概率模型
决策树、朴素贝叶斯、隐马尔可夫模型、条件随机场、概率潜在语义分析、潜在狄利克雷分配、高斯混合模型。

条件概率分布最大化后得到函数，函数归一化后得到条件概率分布。概率模型通常可以表示为联合概率分布的形式，其中的变量表示输入、输出、隐变量甚至参数。而非概率模型则不一定存在这样的联合概率分布。


### 概率图模型
贝叶斯网络、马尔可夫随机场、条件随机场。



## 线性模型与非线性模型（非概率模型）

### 线性模型
感知机、线性支持向量机、k近邻、k均值、潜在语义分析。

### 非线性模型
核函数支持向量机、AdaBoost、神经网络。

## 参数化模型与非参数化模型
### 参数化模型
感知机、朴素贝叶斯、逻辑斯谛回归、k均值、高斯混合模型、潜在语义分析、概率潜在语义分析、潜在狄利克雷分配
### 非参数化模型
决策树、支持向量机、AdaBoost、k近邻。

注：参数化模型适合问题简单的情况，现实中问题往往比较复杂，非参数化模型更加有效。

# 统计学习方法三要素
模型、策略和算法


## 模型
在监督学习过程中，模型就是所要学习的条件概率分布或决策函数。



# 策略
统计学习的目标在于从假设空间中选取最优模型。

首先引入损失函数与风险函数的概念。损失函数度量模型一次预测的好坏，风险函数度量平均意义下模型预测的好坏。

## 损失函数和风险函数
## 算法
指学习模型的具体计算方法。统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优模型。

    这时，统计学习问题归结为最优化问题，统计学习的算法成为求解最优化问题的算法。如果最优化问题有显式的解析解，这个最优化问题就比较简单。但通常解析解不存在，这就需要用数值计算的方法求解。如何保证找到全局最优解，并使求解的过程非常高效，就成为一个重要问题。统计学习可以利用已有的最优化算法，有时也需要开发独自的最优化算法。
统计学习方法之间的不同，主要来自其模型、策略、算法的不同。确定了模型、策略、算法，统计学习的方法也就确定了。
# 监督学习的几个重要概念
## 训练误差与测试误差
训练误差：判断给定问题是否容易学习，但本质上不重要。

测试误差：反映了学习方法对未知的测试数据集的预测能力，是学习中的重要概念。
    
    通常将学习方法对未知数据的预测能力称为泛化能力（generalization ability）​。

## 过拟合与模型选择
过拟合：指学习时选择的模型所包含的参数过多，以至出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。

模型选择旨在避免过拟合并提高模型的预测能力。




<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="./images/1、监督学习/train_test_loss.jpg    ">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">训练误差和测试误差与模型的复杂度之间的关系。</div>
</center>
        
        当选择的模型复杂度过大时，过拟合现象就会发生。这样，在学习时就要防止过拟合，进行最优的模型选择，即选择复杂度适当的模型，以达到使测试误差最小的学习目的。
## 正则化与交叉验证
正则化（regularization）​：模型选择的典型方法。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。

交叉验证（cross validation）​：交叉验证的基本想法是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。

        训练集用来训练模型，验证集用于模型的选择，而测试集用于最终对学习方法的评估。在学习到的不同复杂度的模型中，选择对验证集有最小预测误差的模型。由于验证集有足够多的数据，用它对模型进行选择也是有效的。

### 1．简单交叉验证
简单交叉验证方法是：首先随机地将已给数据分为两部分，一部分作为训练集，另一部分作为测试集（例如，70%的数据为训练集，30%的数据为测试集）​；然后用训练集在各种条件下（例如，不同的参数个数）训练模型，从而得到不同的模型；在测试集上评价各个模型的测试误差，选出测试误差最小的模型。

### 2．S折交叉验证
应用最多的是S折交叉验证（S-fold cross validation）​，方法如下：首先随机地将已给数据切分为S个互不相交、大小相同的子集；然后利用S−1个子集的数据训练模型，利用余下的子集测试模型；将这一过程对可能的S种选择重复进行；最后选出S次评测中平均测试误差最小的模型。

### 3．留一交叉验证
S折交叉验证的特殊情形是S=N，称为留一交叉验证（leave-one-out cross validation）​，往往在数据缺乏的情况下使用。这里，N是给定数据集的容量。

## 生成模型与判别模型

生成方法：原理上由数据学习联合概率分布P（X,Y）​，然后求出条件概率分布P（Y|X）作为预测的模型，即生成模型：P（Y|X）=P(X,Y)/P(X). 这样的方法之所以称为生成方法，是因为模型表示了给定输入X产生输出Y的生成关系。
    
    典型的生成模型有朴素贝叶斯法和隐马尔可夫模型。

判别方法:由数据直接学习决策函数f（X）或者条件概率分布P（Y|X）作为预测的模型，即判别模型。判别方法关心的是对给定的输入X，应该预测什么样的输出Y。
    
    典型的判别模型包括：k近邻法、感知机、逻辑斯谛回归模型、最大熵模型、支持向量机、提升方法和条件随机场等。

生成方法的特点：生成方法可以还原出联合概率分布P（X,Y）​，而判别方法则不能；生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快地收敛于真实模型；当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用。

判别方法的特点：判别方法直接学习的是条件概率P（Y|X）或决策函数f（X）​，直接面对预测，往往学习的准确率更高；由于直接学习P（Y|X）或f（X）​，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。

## 监督学习应用
监督学习的应用主要在三个方面：
    
    分类问题、标注问题和回归问题。
分类是监督学习的一个核心问题。在监督学习中，当输出变量Y取有限个离散值时，预测问题便成为分类问题。


### 分类
分类问题包括学习和分类两个过程。在学习过程中，根据已知的训练数据集利用有效的学习方法学习一个分类器；在分类过程中，利用学习的分类器对新的输入实例进行分类。


对于二类分类问题常用的评价指标是精确率（precision）与召回率（recall）​。通常以关注的类为正类，其他类为负类。
    
    TP——将正类预测为正类数；

    FN——将正类预测为负类数；

    FP——将负类预测为正类数；

    TN——将负类预测为负类数。

精确率定义为：
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="./images/1、监督学习/精确率.jpg    ">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">精确率</div>
</center>


召回率定义为：
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="./images/1、监督学习/召回率.jpg    ">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">召回率</div>
</center>

F1值，是精确率和召回率的调和均值，即：
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="./images/1、监督学习/F1.jpg    ">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    "></div>
</center>











<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="./images/1、监督学习/F1-2.jpg    ">
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">F1值，精确率和召回率的调和均值</div>
</center>
    
    
    统计学习方法中的分类：k近邻法、感知机、朴素贝叶斯法、决策树、决策列表、逻辑斯谛回归模型、支持向量机、提升方法、贝叶斯网络、神经网络、Winnow等。
### 标注问题

标注问题是分类问题的一个推广，标注问题又是更复杂的结构预测（structure prediction）问题的简单形式。\


输入:一个观测序列



输出:一个标记序列或状态序列。


标注问题的目标在于学习一个模型，使它能够对观测序列给出标记序列作为预测。
    
    
    注意，可能的标记个数是有限的，但其组合所成的标记序列的个数是依序列长度呈指数级增长的。

标注常用的统计学习方法有：隐马尔可夫模型、条件随机场。


标注问题在信息抽取、自然语言处理等领域被广泛应用，是这些领域的基本问题。例如，自然语言处理中的词性标注（part of speech tagging）就是一个典型的标注问题：给定一个由单词组成的句子，对这个句子中的每一个单词进行词性标注，即对一个单词序列预测其对应的词性标记序列。

    输入：At Microsoft Research，we have an insatiable curiosity and the desire to create new technology that will help de fine the computing experience.
    
    输出：At/O Microsoft/B Research/E，we/O have/O an/O insatiable/B curiosity/E and/O the/O desire/BE to/O create/O new/B technology/E that/O will/O help/O de fine/O the/O computing/B experience/E.

### 回归问题
回归用于预测输入变量（自变量）和输出变量（因变量）之间的关系。

回归模型表示从输入变量到输出变量之间映射的函数。

回归问题的学习等价于函数拟合：选择一条函数曲线使其很好地拟合已知数据且很好地预测未知数据。

-回归问题按照输入变量的个数，分为一元回归和多元回归；按照输入变量和输出变量之间关系的类型即模型的类型，分为线性回归和非线性回归。

回归问题与预测问题

回归问题和预测问题在数据分析和机器学习领域中经常被提及，它们在某些方面有相似之处，但也存在一些关键的区别：

#### 回归问题：
- 回归问题通常是指预测一个连续的输出值。在回归问题中，我们尝试找到输入变量（特征）和输出变量（目标）之间的关系。
- 回归模型的目标是最小化实际输出值和预测输出值之间的差异，通常使用均方误差（MSE）或均方根误差（RMSE）等指标来衡量这种差异。
- 回归问题的例子包括房价预测、股票价格预测、温度预测等。


#### 预测问题：
- 预测问题是一个更广泛的概念，它包括了回归问题，但不限于此。预测问题可以是预测任何类型的数据，包括连续的数值（回归问题）和离散的类别（分类问题）。



- 预测问题的目标是使用历史数据来预测未来的事件或趋势。
- 预测问题的例子包括天气预测、销售预测、疾病爆发预测等。
#### 回归与预测区别
- ***输出类型***：回归问题专注于预测连续的数值，而预测问题可以预测连续数值也可以预测离散类别。



- ***应用范围***：预测问题的范围更广，它包括了回归问题和分类问题等。


- ***评估指标***：回归问题通常使用连续值的误差指标（如MSE、RMSE）来评估模型性能，而预测问题可能使用不同的指标，如准确率、召回率、F1分数等，这取决于预测的目标。
    
        简而言之，所有的回归问题都可以被视为预测问题，但并非所有的预测问题都是回归问题。预测问题是一个包含回归问题的更大类别。


# 本章概要
1. 统计学习或机器学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行分析与预测的一门学科。统计学习包括监督学习、无监督学习和强化学习。



2. 统计学习方法三要素——模型、策略、算法，对理解统计学习方法起到提纲挈领的作用。
3. 监督学习可以概括如下：从给定有限的训练数据出发，假设数据是独立同分布的，而且假设模型属于某个假设空间，应用某一评价准则，从假设空间中选取一个最优的模型，使它对已给训练数据及未知测试数据在给定评价标准意义下有最准确的预测。




4. 统计学习中，进行模型选择或者说提高学习的泛化能力是一个重要问题。如果只考虑减少训练误差，就可能产生过拟合现象。模型选择的方法有正则化与交叉验证。学习方法泛化能力的分析是统计学习理论研究的重要课题。
5. 分类问题、标注问题和回归问题都是监督学习的重要问题。本书第1篇介绍的统计学习方法包括感知机、k近邻法、朴素贝叶斯法、决策树、逻辑斯谛回归与最大熵模型、支持向量机、提升方法、EM算法、隐马尔可夫模型和条件随机场。这些方法是主要的分类、标注以及回归方法。它们又可以归类为生成方法与判别方法。   



# 习题

* ***1.1***  说明伯努利模型的极大似然估计以及贝叶斯估计中的统计学习方法三要素。伯努利模型是定义在取值为0与1的随机变量上的概率分布。假设观测到伯努利模型n次独立的数据生成结果，其中k次的结果为1，这时可以用极大似然估计或贝叶斯估计来估计结果为1的概率。

        问题分析：假设你有一个伯努利模型，它描述了一个事件的发生与不发生（即结果为1或0）的概率分布。你进行了n次独立的试验（即每次试验之间相互独立，且试验条件相同），并且在这n次试验中，有k次的结果为1（事件发生），其余n-k次的结果为0（事件不发生）。

        问题的核心在于：如何估计事件发生的概率（即结果为1的概率），你可以通过两种统计方法来估计这个概率：极大似然估计（Maximum Likelihood Estimation, MLE）、贝叶斯估计（Bayesian Estimation）

### 伯努利模型的极大似然估计与贝叶斯估计中的统计学习方法三要素

#### 伯努利模型概述
伯努利模型是定义在取值为0与1的随机变量上的概率分布。它表示一个简单的二项分布，其中事件发生的概率为`p`，事件不发生的概率为`1-p`。假设我们观测到该模型`n`次独立的数据生成结果，其中`k`次的结果为1。

#### 极大似然估计（MLE）

##### 1. 模型假设
假设随机变量服从伯努利分布，参数为`p`，即：
$$ P(X=1) = p \quad \text{和} \quad P(X=0) = 1-p $$

##### 2. 似然函数
给定观测数据，极大似然估计法通过最大化似然函数来估计参数。对于伯努利分布，似然函数为：
$$ L(p) = p^k (1-p)^{n-k} $$

##### 3. 参数估计
通过对似然函数取对数并求导数，得到：
$$ \log L(p) = k \log p + (n-k) \log (1-p) $$

对其求导并令其为0，得到极大似然估计：
$$ \frac{k}{p} - \frac{n-k}{1-p} = 0 $$
从而得到结果为1的概率的极大似然估计：
$$ \hat{p}_{\text{MLE}} = \frac{k}{n} $$

#### 贝叶斯估计

##### 1. 模型假设
同样假设随机变量服从伯努利分布，参数为`p`，但在贝叶斯估计中，我们还假设参数`p`本身服从某个先验分布。通常选择Beta分布作为先验分布：
$$ p \sim \text{Beta}(\alpha, \beta) $$
其中，$\alpha$ 和 $\beta$ 是先验分布的参数。

##### 2. 后验分布
根据贝叶斯定理，结合观测数据后，后验分布为：
$$ p|D \sim \text{Beta}(\alpha + k, \beta + n - k) $$

##### 3. 参数估计
贝叶斯估计一般采用后验分布的期望值作为参数的估计值，即：
$$ \hat{p}_{\text{Bayes}} = \frac{\alpha + k}{\alpha + \beta + n} $$

#### 统计学习方法三要素

##### 1. 模型
伯努利模型是一个二项分布，模型的参数是事件发生的概率`p`。在极大似然估计中，模型假设所有观测数据独立同分布。在贝叶斯估计中，进一步假设`p`服从先验分布。

##### 2. 策略
极大似然估计的策略是通过最大化似然函数来选择模型参数`p`，使得观测数据的概率最大化。贝叶斯估计则通过最大化后验概率来选择模型参数，综合考虑了先验信息和观测数据。

##### 3. 算法
在极大似然估计中，通过对似然函数求导数并令其为0来求解参数。在贝叶斯估计中，通过计算后验分布的期望值来求得参数估计。

* ***1.2*** 通过经验风险最小化推导极大似然估计。证明模型是条件概率分布，当损失函数是对数损失函数时，经验风险最小化等价于极大似然估计。











# 经验风险最小化推导极大似然估计

## 1. 引言
在统计学习中，经验风险最小化（Empirical Risk Minimization, ERM）是选择模型的核心原则之一。极大似然估计（Maximum Likelihood Estimation, MLE）是一种经典的统计推断方法。下面将通过经验风险最小化推导极大似然估计，并证明当模型是条件概率分布且损失函数是对数损失函数时，经验风险最小化等价于极大似然估计。

## 2. 经验风险最小化（ERM）

### 2.1 定义
经验风险最小化的目标是在假设的模型集合中，选择一个使得经验风险最小的模型。经验风险定义为在训练数据上的平均损失：
$$
R_{emp}(h) = \frac{1}{n} \sum_{i=1}^{n} L(y_i, h(x_i))
$$
其中，\(h(x)\) 是模型的预测函数，\($y_i$\) 是第 \(i\) 个样本的真实标签，\(L(y, h(x))\) 是损失函数，衡量预测值 \(h(x)\) 与真实值 \(y\) 之间的差距。

### 2.2 目标
经验风险最小化的目标是找到一个模型 \($h^*$\) 使得经验风险最小化：
$$
h^* = \arg \min_{h \in \mathcal{H}} R_{emp}(h)
$$

## 3. 极大似然估计（MLE）

### 3.1 定义
极大似然估计是一种估计统计模型参数的方法，目标是最大化观测数据的似然函数。假设我们有一个模型，其参数为 $\theta$，观测数据为 $\{(x_i, y_i)\}_{i=1}^n$，则似然函数为：
$$
L(\theta) = \prod_{i=1}^{n} P(y_i | x_i; \theta)
$$

极大似然估计通过最大化似然函数来找到参数的估计值 $\hat{\theta}$：
$$
\hat{\theta} = \arg \max_{\theta} L(\theta)
$$

### 3.2 对数似然函数
为了简化计算，通常使用对数似然函数进行优化：
$$
\log L(\theta) = \sum_{i=1}^{n} \log P(y_i | x_i; \theta)
$$

## 4. 经验风险最小化等价于极大似然估计

### 4.1 条件概率分布模型
假设模型 $h(x)$ 是一个条件概率分布$P(y|x;\theta)$，即模型给出了在给定输入 $x$ 的情况下，输出 $y$ 的概率。

### 4.2 对数损失函数
选择对数损失函数 $L(y, h(x))$ 作为损失函数：
$$
L(y, h(x)) = -\log P(y | x; \theta)
$$

### 4.3 经验风险最小化
在此情况下，经验风险变为：
$$
R_{emp}(\theta) = \frac{1}{n} \sum_{i=1}^{n} -\log P(y_i | x_i; \theta)
$$
最小化经验风险等价于最大化负对数损失的和，即最大化对数似然函数：
$$
\hat{\theta} = \arg \min_{\theta} R_{emp}(\theta) = \arg \min_{\theta} \left( -\frac{1}{n} \sum_{i=1}^{n} \log P(y_i | x_i; \theta) \right)
$$
这与极大似然估计中的对数似然函数最大化完全一致。

### 4.4 证明结论
因此，当模型是条件概率分布且损失函数选择为对数损失函数时，经验风险最小化等价于极大似然估计。


## 5. 结论
通过以上推导与证明，可以看到经验风险最小化和极大似然估计之间的紧密联系。当使用条件概率分布模型并选择对数损失函数时，经验风险最小化问题实际上就是极大似然估计问题。这一结论在统计学习和机器学习的理论与应用中具有重要意义。